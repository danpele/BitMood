{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a584b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "# Content:\n",
    "# part I:   Install and import packages and load data\n",
    "# part II:  Pre-process and vectorize the documents\n",
    "# Part III: Training LDA model\n",
    "# Part IV:  Find the optimal number of topics using coherence_values\n",
    "# Part V:   Compute similarity of topics \n",
    "# Part VI:  Visualize the topics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8a9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (2.1.4)\n",
      "Requirement already satisfied: pyfume in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n",
      "Requirement already satisfied: simpful in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in /opt/anaconda3/lib/python3.11/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Part I: install and import packages and load data  \n",
    "\n",
    "##########################################################\n",
    "# please install these modules before you run the code:\n",
    "!pip install gensim\n",
    "\n",
    "!pip install nltk\n",
    "#!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43d090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielpele/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "## `nltk.download('punkt')\n",
    "import numpy as np\n",
    "\n",
    "# NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, sparse2full\n",
    "#from gensim.test.utils import common_corpus\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "#from gensim.models import LdaSeqModel\n",
    "#from gensim.corpora import Dictionary, bleicorpus\n",
    "#from gensim import models, similarities\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import  defaultdict\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2330ecb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Page Name</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Facebook Id</th>\n",
       "      <th>Page Category</th>\n",
       "      <th>Page Admin Top Country</th>\n",
       "      <th>Page Description</th>\n",
       "      <th>Page Created</th>\n",
       "      <th>Likes at Posting</th>\n",
       "      <th>...</th>\n",
       "      <th>Overperforming Score</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>year</th>\n",
       "      <th>predominant_sentiment</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "      <th>final_sentiment_score</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-27 23:07:20</td>\n",
       "      <td>0</td>\n",
       "      <td>Cheat Codes ATL</td>\n",
       "      <td>cheatcodesatlanta</td>\n",
       "      <td>1.000000e+14</td>\n",
       "      <td>ORG_GENERAL</td>\n",
       "      <td>US</td>\n",
       "      <td>Cheat Codes is a media company that empowers p...</td>\n",
       "      <td>25/01/2017 02:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4,283.59</td>\n",
       "      <td>0.051737</td>\n",
       "      <td>0.022936</td>\n",
       "      <td>0.925327</td>\n",
       "      <td>2017</td>\n",
       "      <td>sentiment_neutral</td>\n",
       "      <td>0.028801</td>\n",
       "      <td>14.984698</td>\n",
       "      <td>15.013499</td>\n",
       "      <td>0.028801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-17 00:01:18</td>\n",
       "      <td>1</td>\n",
       "      <td>Batdad</td>\n",
       "      <td>BatDadOfficial</td>\n",
       "      <td>1.000000e+14</td>\n",
       "      <td>DIGITAL_CREATOR</td>\n",
       "      <td>US</td>\n",
       "      <td>I'm BatDad</td>\n",
       "      <td>25/09/2013 01:30</td>\n",
       "      <td>6857356.0</td>\n",
       "      <td>...</td>\n",
       "      <td>184.31</td>\n",
       "      <td>0.043961</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>0.940754</td>\n",
       "      <td>2013</td>\n",
       "      <td>sentiment_neutral</td>\n",
       "      <td>0.028677</td>\n",
       "      <td>13.593631</td>\n",
       "      <td>13.622308</td>\n",
       "      <td>0.028677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-17 02:06:24</td>\n",
       "      <td>2</td>\n",
       "      <td>John-Paul Ugwu</td>\n",
       "      <td>jpaulugwu2</td>\n",
       "      <td>1.000000e+14</td>\n",
       "      <td>PERSONAL_BLOG</td>\n",
       "      <td>NG</td>\n",
       "      <td>We are news Network</td>\n",
       "      <td>14/12/2017 12:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2,790.59</td>\n",
       "      <td>0.038404</td>\n",
       "      <td>0.225466</td>\n",
       "      <td>0.736130</td>\n",
       "      <td>2017</td>\n",
       "      <td>sentiment_neutral</td>\n",
       "      <td>-0.187062</td>\n",
       "      <td>6.258647</td>\n",
       "      <td>6.071585</td>\n",
       "      <td>-0.187062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-01 19:46:35</td>\n",
       "      <td>3</td>\n",
       "      <td>Bitcoin &amp; Crypto</td>\n",
       "      <td>bitcoinandcrypto</td>\n",
       "      <td>1.000000e+14</td>\n",
       "      <td>ACTIVITY_GENERAL</td>\n",
       "      <td>US</td>\n",
       "      <td>Bringing you the latest news and analyses on t...</td>\n",
       "      <td>05/06/2017 19:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>251.8</td>\n",
       "      <td>0.040747</td>\n",
       "      <td>0.030607</td>\n",
       "      <td>0.928647</td>\n",
       "      <td>2017</td>\n",
       "      <td>sentiment_neutral</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>10.901994</td>\n",
       "      <td>10.912135</td>\n",
       "      <td>0.010140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-22 05:56:21</td>\n",
       "      <td>4</td>\n",
       "      <td>LoanCoin</td>\n",
       "      <td>LoanCoin2017</td>\n",
       "      <td>1.000000e+14</td>\n",
       "      <td>TOPIC_BUSINESS_SERVICES</td>\n",
       "      <td>TH</td>\n",
       "      <td>LOANCOIN has designed an advanced \"loan system...</td>\n",
       "      <td>02/12/2017 11:19</td>\n",
       "      <td>176368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>39.22</td>\n",
       "      <td>0.090149</td>\n",
       "      <td>0.070592</td>\n",
       "      <td>0.839259</td>\n",
       "      <td>2017</td>\n",
       "      <td>sentiment_neutral</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>5.525476</td>\n",
       "      <td>5.545033</td>\n",
       "      <td>0.019557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Unnamed: 0         Page Name          User Name  \\\n",
       "0  2017-01-27 23:07:20           0   Cheat Codes ATL  cheatcodesatlanta   \n",
       "1  2017-12-17 00:01:18           1            Batdad     BatDadOfficial   \n",
       "2  2017-12-17 02:06:24           2    John-Paul Ugwu         jpaulugwu2   \n",
       "3  2017-07-01 19:46:35           3  Bitcoin & Crypto   bitcoinandcrypto   \n",
       "4  2017-12-22 05:56:21           4          LoanCoin       LoanCoin2017   \n",
       "\n",
       "    Facebook Id            Page Category Page Admin Top Country  \\\n",
       "0  1.000000e+14              ORG_GENERAL                     US   \n",
       "1  1.000000e+14          DIGITAL_CREATOR                     US   \n",
       "2  1.000000e+14            PERSONAL_BLOG                     NG   \n",
       "3  1.000000e+14         ACTIVITY_GENERAL                     US   \n",
       "4  1.000000e+14  TOPIC_BUSINESS_SERVICES                     TH   \n",
       "\n",
       "                                    Page Description      Page Created  \\\n",
       "0  Cheat Codes is a media company that empowers p...  25/01/2017 02:20   \n",
       "1                                         I'm BatDad  25/09/2013 01:30   \n",
       "2                                We are news Network  14/12/2017 12:27   \n",
       "3  Bringing you the latest news and analyses on t...  05/06/2017 19:19   \n",
       "4  LOANCOIN has designed an advanced \"loan system...  02/12/2017 11:19   \n",
       "\n",
       "   Likes at Posting  ...  Overperforming Score sentiment_positive  \\\n",
       "0               NaN  ...              4,283.59           0.051737   \n",
       "1         6857356.0  ...                184.31           0.043961   \n",
       "2               NaN  ...              2,790.59           0.038404   \n",
       "3               NaN  ...                 251.8           0.040747   \n",
       "4          176368.0  ...                 39.22           0.090149   \n",
       "\n",
       "  sentiment_negative sentiment_neutral  year  predominant_sentiment  \\\n",
       "0           0.022936          0.925327  2017      sentiment_neutral   \n",
       "1           0.015284          0.940754  2013      sentiment_neutral   \n",
       "2           0.225466          0.736130  2017      sentiment_neutral   \n",
       "3           0.030607          0.928647  2017      sentiment_neutral   \n",
       "4           0.070592          0.839259  2017      sentiment_neutral   \n",
       "\n",
       "   compound_sentiment  sentiment_intensity  final_sentiment_score  compound  \n",
       "0            0.028801            14.984698              15.013499  0.028801  \n",
       "1            0.028677            13.593631              13.622308  0.028677  \n",
       "2           -0.187062             6.258647               6.071585 -0.187062  \n",
       "3            0.010140            10.901994              10.912135  0.010140  \n",
       "4            0.019557             5.525476               5.545033  0.019557  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# Please change the working directory to your path!\n",
    "# os.chdir(\"/Users/xinwenni/LDA-DTM/DTM\") \n",
    "############################\n",
    "\n",
    "# load data\n",
    "\n",
    "path_datasets=\"/Users/danielpele/Library/CloudStorage/GoogleDrive-danpele@ase.ro/Other \\\n",
    "computers/Asus/G/PROIECTE/Understanding Digital Assets/Cod/Datasets/\"\n",
    "# load data\n",
    "df = pd.read_csv(path_datasets+\"posts.csv\", low_memory=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be84a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180 seconds of basic economics explained perfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bitcoin Insta - batdadblake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Today is the saddest day of my life. As a Doct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well people can stop saying \"What can you buy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Investors, 1) Please do not share your pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82805</th>\n",
       "      <td>Bitcoin ATM scams are on the rise, and scammer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82806</th>\n",
       "      <td>Leaving coinbase... but staying in crypto #fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82807</th>\n",
       "      <td>At AES, our main value is Safety first. Here's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82808</th>\n",
       "      <td>Want to accept Bitcoin and Ethereum tips on Tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82809</th>\n",
       "      <td>üîó Curious about what makes up a blockchain? üß± ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82810 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Message\n",
       "0      180 seconds of basic economics explained perfe...\n",
       "1                            Bitcoin Insta - batdadblake\n",
       "2      Today is the saddest day of my life. As a Doct...\n",
       "3      Well people can stop saying \"What can you buy ...\n",
       "4      Dear Investors, 1) Please do not share your pe...\n",
       "...                                                  ...\n",
       "82805  Bitcoin ATM scams are on the rise, and scammer...\n",
       "82806  Leaving coinbase... but staying in crypto #fin...\n",
       "82807  At AES, our main value is Safety first. Here's...\n",
       "82808  Want to accept Bitcoin and Ethereum tips on Tw...\n",
       "82809  üîó Curious about what makes up a blockchain? üß± ...\n",
       "\n",
       "[82810 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad28f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: DeprecationWarning: invalid escape sequence '\\S'\n",
      "<>:16: DeprecationWarning: invalid escape sequence '\\s'\n",
      "<>:13: DeprecationWarning: invalid escape sequence '\\S'\n",
      "<>:16: DeprecationWarning: invalid escape sequence '\\s'\n",
      "/var/folders/pk/sxdc5n5x6mb322grbvxz_nm40000gp/T/ipykernel_4417/3450740866.py:13: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "/var/folders/pk/sxdc5n5x6mb322grbvxz_nm40000gp/T/ipykernel_4417/3450740866.py:16: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['180 seconds of basic economics explained perfectly! Get $10 in Bitcoin FREE '\n",
      " 'when you invest $100 (click our affiliate link): '\n",
      " 'http://bit.ly/coinbasesign:=:https://www.coinbase.com/join/59e80734c90f890144c1f448 '\n",
      " 'Inbox us if you would like anything posted on our page‚ÄºÔ∏è üëåüèæüëåüèæüíØüíØ‚úîÔ∏è Credit: '\n",
      " 'Sean Shewmake (aka \"Tommy Bottoms\")']\n",
      "Number of unique tokens: 61050\n",
      "Number of documents: 82810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##########################################################\n",
    "\n",
    "# Part II: Pre-process and vectorize the documents\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Convert the 'Message' column to a list\n",
    "data = df['Message'].values.tolist()\n",
    "data = [str(item) for item in data]\n",
    "\n",
    "def clean_data(data):\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "    \n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "    \n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "    \n",
    "    pprint(data[:1])\n",
    "    return data\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def get_lemm(data):\n",
    "    data_words = list(sent_to_words(data))\n",
    "#    print(data_words[:1])\n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    \n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    # See trigram example\n",
    "#    print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops=remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "#    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    #nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    #data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "#    print(data_lemmatized[:1])\n",
    "    return data_lemmatized\n",
    "\n",
    "# simple clean the data first \n",
    "data=clean_data(data)\n",
    "# define stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'use','also'])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Tokenize and lemmatize data \n",
    "data_lemmatized=get_lemm(data)\n",
    "\n",
    "# Create Dictionary\n",
    "id2word= corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "#Let‚Äôs see how many tokens and documents we have to train on.\n",
    "print('Number of unique tokens: %d' % len(id2word))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142a4357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -4.1269.\n",
      "[([(0.016716084, 'make'),\n",
      "   (0.013122917, 'go'),\n",
      "   (0.011146025, 'money'),\n",
      "   (0.01026498, 'get'),\n",
      "   (0.009975685, 'good'),\n",
      "   (0.00879592, 'take'),\n",
      "   (0.0082691405, 'people'),\n",
      "   (0.0076578264, 'say'),\n",
      "   (0.0075277034, 'call'),\n",
      "   (0.0075157243, 'work'),\n",
      "   (0.0073094256, 'know'),\n",
      "   (0.006598142, 'come'),\n",
      "   (0.006498109, 'world'),\n",
      "   (0.0063479817, 'give'),\n",
      "   (0.0060525327, 'need'),\n",
      "   (0.0059881657, 'year'),\n",
      "   (0.005910892, 'time'),\n",
      "   (0.0056609996, 'well'),\n",
      "   (0.0055262637, 'many'),\n",
      "   (0.005207671, 'account')],\n",
      "  -1.834068231663089),\n",
      " ([(0.0786702, 'bitcoin'),\n",
      "   (0.029184207, 'market'),\n",
      "   (0.023897715, 'price'),\n",
      "   (0.023061687, 'crypto'),\n",
      "   (0.011830853, 'day'),\n",
      "   (0.011721596, 'week'),\n",
      "   (0.011067428, 'high'),\n",
      "   (0.009543237, 'year'),\n",
      "   (0.008458293, 'time'),\n",
      "   (0.0077778283, 'last'),\n",
      "   (0.007667661, 'today'),\n",
      "   (0.0072346837, 'month'),\n",
      "   (0.0066127367, 'move'),\n",
      "   (0.006407395, 'see'),\n",
      "   (0.006251052, 'level'),\n",
      "   (0.006196546, 'investor'),\n",
      "   (0.006149914, 'ethereum'),\n",
      "   (0.006128463, 'low'),\n",
      "   (0.006089945, 'cryptocurrency'),\n",
      "   (0.0057993997, 'top')],\n",
      "  -2.2883846468586344),\n",
      " ([(0.028074645, 'trading'),\n",
      "   (0.025459971, 'crypto'),\n",
      "   (0.02293539, 'cryptocurrency'),\n",
      "   (0.016498987, 'investment'),\n",
      "   (0.016000528, 'trade'),\n",
      "   (0.015444235, 'exchange'),\n",
      "   (0.013570699, 'coin'),\n",
      "   (0.013188129, 'blockchain'),\n",
      "   (0.010614866, 'invest'),\n",
      "   (0.01058434, 'network'),\n",
      "   (0.0100582065, 'mining'),\n",
      "   (0.009487452, 'new'),\n",
      "   (0.009411523, 'future'),\n",
      "   (0.009035707, 'platform'),\n",
      "   (0.008914303, 'user'),\n",
      "   (0.008497666, 'wallet'),\n",
      "   (0.008168226, 'token'),\n",
      "   (0.0076078614, 'start'),\n",
      "   (0.0074961316, 'earn'),\n",
      "   (0.0071490156, 'web')],\n",
      "  -2.4682832572711755),\n",
      " ([(0.02129024, 'company'),\n",
      "   (0.019747987, 'currency'),\n",
      "   (0.017224591, 'digital'),\n",
      "   (0.015812606, 'service'),\n",
      "   (0.015344598, 'asset'),\n",
      "   (0.014651359, 'bank'),\n",
      "   (0.012934366, 'financial'),\n",
      "   (0.012763492, 'include'),\n",
      "   (0.0115983635, 'say'),\n",
      "   (0.011300899, 'report'),\n",
      "   (0.0106482385, 'payment'),\n",
      "   (0.010368896, 'fund'),\n",
      "   (0.009530329, 'state'),\n",
      "   (0.009318687, 'country'),\n",
      "   (0.008788819, 'government'),\n",
      "   (0.008293535, 'public'),\n",
      "   (0.0076628104, 'accord'),\n",
      "   (0.0075535355, 'transfer'),\n",
      "   (0.007331538, 'legal'),\n",
      "   (0.0073268944, 'firm')],\n",
      "  -2.473183693613894),\n",
      " ([(0.21976829, 'com'),\n",
      "   (0.07602708, 'https'),\n",
      "   (0.029284766, 'www'),\n",
      "   (0.023657016, 'twitter'),\n",
      "   (0.022715783, 'referral'),\n",
      "   (0.021856733, 'youtube'),\n",
      "   (0.021427682, 'patreon'),\n",
      "   (0.020476038, 'video'),\n",
      "   (0.01564361, 'link'),\n",
      "   (0.013869623, 'channel'),\n",
      "   (0.013588115, 'facebook'),\n",
      "   (0.012931343, 'watch'),\n",
      "   (0.010874286, 'join'),\n",
      "   (0.0101686455, 'bitrue'),\n",
      "   (0.010147552, 'store'),\n",
      "   (0.009666573, 'app'),\n",
      "   (0.009284123, 'support'),\n",
      "   (0.008801773, 'instagram'),\n",
      "   (0.008468629, 'get'),\n",
      "   (0.007845118, 'bit')],\n",
      "  -2.814835174216692),\n",
      " ([(0.035062492, 'beat'),\n",
      "   (0.029087823, 'code'),\n",
      "   (0.021384941, 'affiliate'),\n",
      "   (0.020681323, 'card'),\n",
      "   (0.019126846, 'type'),\n",
      "   (0.01727379, 'free'),\n",
      "   (0.01611821, 'tv'),\n",
      "   (0.016044281, 'payment'),\n",
      "   (0.01531849, 'get'),\n",
      "   (0.011346734, 'buy'),\n",
      "   (0.010997973, 'club'),\n",
      "   (0.010534517, 'gift'),\n",
      "   (0.010118602, 'online'),\n",
      "   (0.009961825, 'credit'),\n",
      "   (0.009891598, 'car'),\n",
      "   (0.009540399, 'ticket'),\n",
      "   (0.009319497, 'project'),\n",
      "   (0.008975284, 'purchase'),\n",
      "   (0.008549908, 'sale'),\n",
      "   (0.008430339, 'paypal')],\n",
      "  -3.1672539136506095),\n",
      " ([(0.020918025, 'right'),\n",
      "   (0.017615305, 'self'),\n",
      "   (0.014494972, 'freedom'),\n",
      "   (0.012627167, 'determination'),\n",
      "   (0.012088647, 'course'),\n",
      "   (0.0120642185, 'matter'),\n",
      "   (0.011153257, 'support'),\n",
      "   (0.010767331, 'school'),\n",
      "   (0.009661359, 'news'),\n",
      "   (0.008914449, 'indigenous'),\n",
      "   (0.008537181, 'student'),\n",
      "   (0.008238945, 'present'),\n",
      "   (0.008128943, 'nation'),\n",
      "   (0.008023465, 'join'),\n",
      "   (0.007743693, 'human'),\n",
      "   (0.0077055027, 'chief'),\n",
      "   (0.007657896, 'training'),\n",
      "   (0.0074305497, 'member'),\n",
      "   (0.006959995, 'compensation'),\n",
      "   (0.006639832, 'culture')],\n",
      "  -3.2846262833795423),\n",
      " ([(0.046806857, 'event'),\n",
      "   (0.026076635, 'win'),\n",
      "   (0.02179622, 'live'),\n",
      "   (0.016262615, 'winner'),\n",
      "   (0.012909016, 'twitter'),\n",
      "   (0.012551345, 'day'),\n",
      "   (0.012116885, 'conference'),\n",
      "   (0.011809434, 'chance'),\n",
      "   (0.011481416, 'giveaway'),\n",
      "   (0.011271115, 'host'),\n",
      "   (0.011131881, 'prize'),\n",
      "   (0.010885386, 'skywalker'),\n",
      "   (0.010786479, 'competition'),\n",
      "   (0.010384095, 'magazine'),\n",
      "   (0.010331825, 'content'),\n",
      "   (0.009057192, 'share'),\n",
      "   (0.008248684, 'participate'),\n",
      "   (0.008180979, 'follow'),\n",
      "   (0.007747707, 'guest'),\n",
      "   (0.0076947426, 'final')],\n",
      "  -4.129906011499731),\n",
      " ([(0.079222426, 'game'),\n",
      "   (0.06779631, 'old'),\n",
      "   (0.027043767, 'gaming'),\n",
      "   (0.021598116, 'song'),\n",
      "   (0.017924389, 'note'),\n",
      "   (0.015983501, 'player'),\n",
      "   (0.013279302, 'travel'),\n",
      "   (0.010589861, 'ride'),\n",
      "   (0.010008416, 'exibition'),\n",
      "   (0.009768572, 'star'),\n",
      "   (0.009453836, 'battle'),\n",
      "   (0.008607124, 'cool'),\n",
      "   (0.008456374, 'motivation'),\n",
      "   (0.0074010915, '‡§ï‡§ï'),\n",
      "   (0.007349013, 'score'),\n",
      "   (0.007016118, 'video'),\n",
      "   (0.0069734906, 'show'),\n",
      "   (0.006743228, 'creator'),\n",
      "   (0.0065430757, 'bsbt'),\n",
      "   (0.0062543564, 'indian')],\n",
      "  -8.57735348420614),\n",
      " ([(0.04227123, 'dogecoin'),\n",
      "   (0.02147219, 'next'),\n",
      "   (0.02040038, 'instrumental'),\n",
      "   (0.01713596, 'partnering'),\n",
      "   (0.017062172, 'road'),\n",
      "   (0.014278418, 'city'),\n",
      "   (0.013747178, 'office'),\n",
      "   (0.011764804, 'digitalcurrency'),\n",
      "   (0.009735789, 'complex'),\n",
      "   (0.009494217, 'park'),\n",
      "   (0.00910125, 'art'),\n",
      "   (0.008879153, 'mall'),\n",
      "   (0.008609769, 'laptop'),\n",
      "   (0.007740662, 'php'),\n",
      "   (0.0077106236, 'btcs'),\n",
      "   (0.007289611, 'batch'),\n",
      "   (0.0070753405, 'cat'),\n",
      "   (0.006670416, 'certify'),\n",
      "   (0.0064999545, 'honble'),\n",
      "   (0.006481134, 'centre')],\n",
      "  -10.231050245634119)]\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "\n",
    "# Part III: Training LDA model \n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "#\n",
    "## Make a index to word dictionary.\n",
    "#temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "#id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "pprint(top_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "\n",
    "# Part IV: find the optimal number of topics using coherence_values\n",
    "\n",
    "##########################################################\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "#        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model = gensim.models.ldamodel.LdaModel( corpus=corpus, num_topics=num_topics, id2word=id2word,random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=15,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=1, limit=11, step=1)\n",
    "\n",
    "# Show graph\n",
    "filename='Num_Topic_CV'\n",
    "limit=11; start=1; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "#plt.ylim([0.25,0.45])\n",
    "#plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.savefig(filename,dpi = 720,transparent=True)\n",
    "plt.show()\n",
    "\n",
    "# optimal number is 11, then adjust the topic number, and retrain the LDA model \n",
    "num_topic=5\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topic, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=15,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True,\n",
    "                                           minimum_probability=0.0)\n",
    "\n",
    "\n",
    "lda_model.show_topics()\n",
    "\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"lda_model\")\n",
    "lda_model.save(temp_file)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics(0,10))\n",
    "doc_lda = lda_model[corpus]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a19f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "\n",
    "# Part V: Compute similarity of topics \n",
    "\n",
    "##########################################################\n",
    "\n",
    "def plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"Helper function to plot difference between models.\n",
    "\n",
    "    Uses matplotlib as the backend.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(18, 14))\n",
    "    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(data)\n",
    "\n",
    "plot_difference = plot_difference_matplotlib\n",
    "'''\n",
    "try:\n",
    "    get_ipython()\n",
    "    import plotly.offline as py\n",
    "except Exception:\n",
    "    #\n",
    "    # Fall back to matplotlib if we're not in a notebook, or if plotly is\n",
    "    # unavailable for whatever reason.\n",
    "    #\n",
    "    plot_difference = plot_difference_matplotlib\n",
    "else:\n",
    "    py.init_notebook_mode()\n",
    "    plot_difference = plot_difference_plotly\n",
    "'''    \n",
    "mdiff, annotation = lda_model.diff(lda_model, distance='hellinger', num_words=50)\n",
    "plot_difference(mdiff,  annotation=annotation)\n",
    "plt.tick_params(labelsize=23)\n",
    "plt.savefig(\"topic_distance_H.png\",dpi = 360,transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eca65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################\n",
    "\n",
    "# Part VI: Visualize the topics\n",
    "\n",
    "##########################################################\n",
    "import pyLDAvis.gensim_models\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ecd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'LDA_vis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2261b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
